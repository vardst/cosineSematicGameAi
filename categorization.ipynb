{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import tiktoken\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "CHAT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "openai.api_key = os.environ.get('api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_managers = pd.read_csv('managers.csv')\n",
    "df_managers = df_managers.set_index(['title','heading'])\n",
    "\n",
    "df_state = pd.read_csv('state.csv')\n",
    "df_state = df_state.set_index(['title','heading'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings part\n",
    "\n",
    "\n",
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }\n",
    "    \n",
    "def load_embeddings(fname: str) -> dict[tuple[str, str], list[float]]:\n",
    "    df = pd.read_csv(fname, header=0, encoding='utf-8')\n",
    "    df = df.set_index(['title','heading'])\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"title\" and c != \"heading\"])\n",
    "    return {\n",
    "           (r.title, r.heading): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n",
    "    }\n",
    "\n",
    "def get_usage(text: str, model: str = EMBEDDING_MODEL)->int:\n",
    "    result = openai.Embedding.create(\n",
    "        model=model,\n",
    "        imput = text\n",
    "    )\n",
    "    return result[\"usage\"][\"total_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers_embeddings = compute_doc_embeddings(df_managers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_embedding = compute_doc_embeddings(df_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Square Sam', 'unlock level = 45') : [-0.00020701090397778898, -0.008801721036434174, 0.00948029663413763, -0.02485036663711071, -0.016799692064523697]... (1536 entries)\n",
      "('mine areas', 8) : [-0.01969183050096035, -0.01719668135046959, -0.004136134870350361, -0.026822851970791817, -0.02761676348745823]... (1536 entries)\n"
     ]
    }
   ],
   "source": [
    "#show embeddings\n",
    "\n",
    "example_entry = list(managers_embeddings.items())[-1]\n",
    "print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")\n",
    "example_entry = list(state_embedding.items())[-1]\n",
    "print(f\"{example_entry[0]} : {example_entry[1][:5]}... ({len(example_entry[1])} entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector similarity and user prompt embedding \n",
    "\n",
    "\n",
    "def vector_similarity(x: list[float],y: list[float])->float:\n",
    "    return np.dot(np.array(x),np.array(y))\n",
    "\n",
    "def order_document_selections_by_querry_similarity(query:str , contexts: dict[(str,str),np.array])->list[(float,(str, str))]:\n",
    "    query_embedding = get_embedding(query)\n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding),doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ],reverse = True)\n",
    "    return document_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manager(prompt)-> list[str,float]:\n",
    "    manager_guess = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:1][0][1][0]\n",
    "    manager_chance = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:1][0][0]*100\n",
    "    return [manager_guess,manager_chance]\n",
    "\n",
    "def get_second_manager(prompt)-> list[str,float]:\n",
    "    manager_guess = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:2][1][1][0]\n",
    "    manager_chance = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:2][1][0]*100\n",
    "    return [manager_guess,manager_chance]\n",
    "    \n",
    "    manager_guess = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:1][0][1][0]\n",
    "    manager_chance = order_document_selections_by_querry_similarity(prompt,managers_embeddings)[:1][0][0]*100\n",
    "\n",
    "    state_guess = order_document_selections_by_querry_similarity(prompt,state_embedding)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LillyPI', 82.35284145030951]\n",
      "['Timmy', 81.91692214070527]\n"
     ]
    }
   ],
   "source": [
    "print(get_manager(\"Who is Timmy's Crush ?\"))\n",
    "print(get_second_manager(\"Who is Timmy's Crush ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"prompt\",'r',encoding='utf-8') as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_messages(messages,role,content):\n",
    "    messages.append({\"role\":role,\"content\":content})\n",
    "    return messages\n",
    "\n",
    "def get_response_messages(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model = CHAT_MODEL,\n",
    "    messages=messages\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt,}\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_df(text,df):\n",
    "    content = df.loc[f'{text}',\"content\"]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    querry = input()\n",
    "    print(querry)\n",
    "    guessed_manager = get_manager(querry)[0]\n",
    "    get_info = get_text_from_df(guessed_manager,df_managers)\n",
    "    messages = update_messages(messages=messages,role = \"user\",content = querry)\n",
    "    messages = update_messages(messages=messages, role=\"system\" , content=f\"{get_info}\")\n",
    "    model_response = get_response_messages(messages)\n",
    "    print(model_response)\n",
    "    messages = update_messages(messages=messages,role = \"assistant\", content=model_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
